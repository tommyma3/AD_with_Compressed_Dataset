# Multi-GPU Training Guide

This guide explains how to train your Algorithm Distillation model using multiple GPUs for faster training.

## Quick Start

### Option 1: Using `accelerate launch` (Recommended)

```bash
# First, configure accelerate (one-time setup)
accelerate config

# Then launch training with all available GPUs
accelerate launch train.py

# Or specify number of GPUs
accelerate launch --num_processes 2 train.py

# Or specify which GPUs to use
accelerate launch --gpu_ids 0,1 train.py
```

### Option 2: Using `torchrun` (PyTorch Distributed)

```bash
# Launch on 2 GPUs
torchrun --nproc_per_node=2 train.py

# Launch on 4 GPUs
torchrun --nproc_per_node=4 train.py

# Launch on specific GPUs
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 train.py
```

### Option 3: Single GPU (Default)

```bash
# Just run normally - will use first available GPU
python train.py
```

## Configuration

### Accelerate Configuration (One-time Setup)

Run `accelerate config` and answer the prompts:

```
In which compute environment are you running?
> This machine

Which type of machine are you using?
> multi-GPU

How many different machines will you use?
> 1

Should distributed operations be checked while running for errors?
> yes

Do you wish to optimize your script with torch dynamo?
> no

Do you want to use DeepSpeed?
> no

Do you want to use FullyShardedDataParallel?
> no

Do you want to use Megatron-LM?
> no

How many GPU(s) should be used for distributed training?
> [Enter number of GPUs, e.g., 2, 4, 8]

What GPU(s) (by id) should be used for training on this machine?
> all  [or specify: 0,1,2,3]

Do you wish to use FP16 or BF16 (mixed precision)?
> fp16  [recommended for faster training]
```

This creates `~/.cache/huggingface/accelerate/default_config.yaml`

### Model Configuration

Edit your config file (e.g., `config/model/ad_dr_compressed.yaml`):

```yaml
# Multi-GPU training settings
mixed_precision: 'fp16'  # Options: 'no', 'fp16', 'bf16'
gradient_accumulation_steps: 1  # Increase if you run out of memory

# Adjust batch size based on GPU memory
train_batch_size: 512  # Per GPU batch size
test_batch_size: 2048
```

## Key Features

### 1. Automatic GPU Distribution
- Model parameters are automatically distributed across GPUs
- Data is automatically split across processes
- Gradients are synchronized automatically

### 2. Mixed Precision Training
Enable faster training with reduced memory:

```yaml
mixed_precision: 'fp16'  # 16-bit floating point
# or
mixed_precision: 'bf16'  # BF16 (better for some models, requires Ampere+ GPUs)
```

**Benefits:**
- ~2x faster training
- ~50% less memory usage
- Minimal accuracy loss

### 3. Gradient Accumulation
Simulate larger batch sizes on limited memory:

```yaml
gradient_accumulation_steps: 4  # Effective batch size = 512 * 4 = 2048
```

**Use when:**
- You want larger effective batch size but run out of memory
- You have limited GPU memory
- You want to maintain the same learning dynamics as larger batches

### 4. Process Synchronization
- Checkpoints saved only from main process
- Logging happens only on main process
- All processes synchronized before checkpoint saving
- Evaluation runs only on main process (no redundant computation)

## Performance Optimization

### Recommended Settings by GPU Count

#### 2 GPUs (e.g., 2x RTX 3090)
```yaml
mixed_precision: 'fp16'
gradient_accumulation_steps: 1
train_batch_size: 512
num_workers: 4
```

Expected speedup: ~1.8x

#### 4 GPUs (e.g., 4x A100)
```yaml
mixed_precision: 'fp16'
gradient_accumulation_steps: 1
train_batch_size: 256  # Reduce per-GPU batch size
num_workers: 4
```

Expected speedup: ~3.5x

#### 8 GPUs (Large cluster)
```yaml
mixed_precision: 'fp16'
gradient_accumulation_steps: 2
train_batch_size: 128  # Smaller per-GPU batch size
num_workers: 4
```

Expected speedup: ~6-7x

### Memory Management

If you encounter OOM (Out of Memory) errors:

1. **Reduce batch size:**
   ```yaml
   train_batch_size: 256  # Instead of 512
   ```

2. **Increase gradient accumulation:**
   ```yaml
   gradient_accumulation_steps: 2  # Maintain effective batch size
   ```

3. **Reduce model size:**
   ```yaml
   tf_n_embd: 32  # Instead of 64
   tf_n_layer: 2  # Instead of 4
   ```

4. **Reduce context length:**
   ```yaml
   n_transit: 160  # Instead of 240
   ```

## Monitoring Training

### View Multi-GPU Training Stats

During training, you'll see:
```
============================================================
Multi-GPU Training Configuration
============================================================
Number of processes: 4
Distributed type: DistributedType.MULTI_GPU
Mixed precision: fp16
Gradient accumulation steps: 1
Main process device: cuda:0
============================================================
```

### TensorBoard

```bash
tensorboard --logdir runs/
```

Metrics are logged only from the main process to avoid duplicates.

### Progress Bar

Only the main process shows the progress bar to avoid clutter.

## Troubleshooting

### Issue: "CUDA out of memory"

**Solutions:**
1. Reduce `train_batch_size`
2. Increase `gradient_accumulation_steps`
3. Use `mixed_precision: 'fp16'`
4. Reduce `n_transit` or model size

### Issue: "Training is slower with multiple GPUs"

**Causes:**
- Batch size too small (communication overhead dominates)
- Data loading bottleneck

**Solutions:**
1. Increase `train_batch_size` per GPU
2. Increase `num_workers` for data loading
3. Use faster storage (SSD) for datasets
4. Enable `mixed_precision`

### Issue: "Different results on different runs"

**Cause:** Non-deterministic GPU operations

**Solution:** Set deterministic mode (may be slower):
```python
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```

### Issue: "Hanging or stalling during training"

**Causes:**
- Process synchronization issues
- Networking problems (multi-node)

**Solutions:**
1. Check all GPUs are visible: `nvidia-smi`
2. Verify NCCL is working: `python -c "import torch; torch.cuda.nccl.version()"`
3. Check network connectivity (multi-node setups)
4. Try: `export NCCL_DEBUG=INFO` for verbose logging

### Issue: "Checkpoint loading fails"

**Cause:** Model wrapped by DDP, state dict keys have 'module.' prefix

**Solution:** Already handled in code with `accelerator.unwrap_model()`

## Advanced: Multi-Node Training

For training across multiple machines:

1. **Configure accelerate for multi-node:**
   ```bash
   accelerate config
   # Select: multi-GPU
   # How many machines: [number of nodes]
   # What is the rank of this machine: [0 for main, 1, 2, ... for others]
   # What is the IP address of the main machine: [main node IP]
   # What is the port of the main machine: [e.g., 29500]
   ```

2. **Launch on main node (rank 0):**
   ```bash
   accelerate launch train.py
   ```

3. **Launch on worker nodes (rank 1, 2, ...):**
   ```bash
   accelerate launch train.py
   ```

4. **Or use SLURM (cluster environments):**
   ```bash
   sbatch multi_gpu_train.slurm
   ```

## Performance Comparison

Expected training time for 50k steps on darkroom:

| Setup | Time | Speedup |
|-------|------|---------|
| 1x RTX 3090 | ~8 hours | 1.0x |
| 2x RTX 3090 | ~4.5 hours | 1.8x |
| 4x A100 | ~2.5 hours | 3.2x |
| 8x A100 | ~1.5 hours | 5.3x |

*With mixed_precision='fp16' and optimal batch sizes*

## Best Practices

1. **Always use mixed precision** (`fp16` or `bf16`) for faster training
2. **Monitor GPU utilization** with `nvidia-smi` or `nvtop`
3. **Start with 2 GPUs** to verify setup works before scaling
4. **Use gradient accumulation** to simulate larger batches
5. **Save checkpoints frequently** (every 1000 steps) for long training runs
6. **Keep batch size per GPU reasonable** (256-512 for most models)
7. **Profile your code** to identify bottlenecks:
   ```python
   with torch.profiler.profile() as prof:
       # training step
   print(prof.key_averages().table(sort_by="cuda_time_total"))
   ```

## Example Launch Scripts

### Simple Multi-GPU Launch
```bash
#!/bin/bash
# train_multi_gpu.sh

export CUDA_VISIBLE_DEVICES=0,1,2,3
accelerate launch --num_processes 4 train.py
```

### SLURM Cluster
```bash
#!/bin/bash
#SBATCH --job-name=ad_train
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --gres=gpu:4
#SBATCH --time=24:00:00

module load cuda/11.8
module load python/3.10

source .venv/bin/activate
accelerate launch train.py
```

### With Custom Config
```bash
#!/bin/bash
# Override config in train.py first, then:
accelerate launch \
    --mixed_precision fp16 \
    --num_processes 4 \
    train.py
```

## Verification

After setup, verify multi-GPU is working:

1. **Check GPU usage during training:**
   ```bash
   watch -n 1 nvidia-smi
   ```
   You should see all specified GPUs at high utilization.

2. **Check training output:**
   Look for "Number of processes: N" in the startup messages.

3. **Check model size:**
   Model parameters should be distributed/replicated across GPUs.

4. **Check performance:**
   Training should be faster with more GPUs (diminishing returns beyond 4-8 GPUs).

## Summary

Multi-GPU training with `accelerate` is:
- âœ… **Easy to use**: Just add `accelerate launch` before your command
- âœ… **Automatic**: Data distribution and gradient synchronization handled automatically
- âœ… **Flexible**: Works with single GPU, multi-GPU, or multi-node
- âœ… **Efficient**: Near-linear speedup for most workloads
- âœ… **Compatible**: Works with existing code (minimal changes needed)

For most users, start with:
```bash
accelerate config  # One-time setup
accelerate launch train.py  # Use all GPUs
```

That's it! ðŸš€
